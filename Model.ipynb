{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\n\n# Replace \"path_to_your_file/train.csv\" with the actual path to your file\nfile_path = 'train.csv'\n\n# Reading the training data\ntrain_data = pd.read_csv('train.csv')\n\n\n# Displaying the first few rows of the data\nprint(train_data.head())\n# Handling missing values\ntrain_data = train_data.dropna()  # This will drop rows with any missing values\n# Alternatively, you can impute missing values using train_data.fillna() method\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "   id   x1   x2                                            x3  \\\n0   1   NO   NO  dqOiM6yBYgnVSezBRiQXs9bvOFnRqrtIoXRIElxD7g8=   \n1   2  NaN  NaN                                           NaN   \n2   3   NO   NO  ib4VpsEsqJHzDiyL0dZLQ+xQzDPrkxE+9T3mx5fv2wI=   \n3   4  YES   NO  BfrqME7vdLw3suQp6YAT16W2piNUmpKhMzuDrVrFQ4w=   \n4   5   NO   NO  RTjsrrR8DTlJyaIP9Q3Z8s0zseqlVQTrlSe97GCWfbk=   \n\n                                             x4        x5        x6        x7  \\\n0  GNjrXXA3SxbgD0dTRblAPO9jFJ7AIaZnu/f48g5XSUk=  0.576561  0.073139  0.481394   \n1                                           NaN  0.000000  0.000000  0.000000   \n2  X6dDAI/DZOWvu0Dg6gCgRoNr2vTUz/mc4SdHTNUPS38=  1.341803  0.051422  0.935572   \n3  YGCdISifn4fLao/ASKdZFhGIq23oqzfSbUVb6px1pig=  0.653912  0.041471  0.940787   \n4  3yK2OPj1uYDsoMgsxsjY1FxXkOllD8Xfh20VYGqT+nU=  1.415919  0.000000  1.000000   \n\n         x8        x9  ... x136   x137  x138  x139 x140  x141  x142  x143  \\\n0  0.115697  0.472474  ...  0.0  0.810  3306  4676  YES    NO   YES     2   \n1  0.000000  0.000000  ...  0.0  0.510  4678  3306  YES    NO   YES     4   \n2  0.041440  0.501710  ...  0.0  0.850  4678  3306   NO    NO    NO     1   \n3  0.090851  0.556564  ...  0.0  0.945  3306  4678   NO    NO   YES     3   \n4  0.000000  0.375297  ...  0.0  1.000  1263   892   NO    NO    NO     1   \n\n       x144      x145  \n0  0.375535  0.464610  \n1  0.741682  0.593630  \n2  0.776467  0.493159  \n3  0.168234  0.546582  \n4  0.246637  0.361045  \n\n[5 rows x 146 columns]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": "# For example, you can use the following for handling missing values:\ntrain_data = train_data.fillna(0)  # Replace NaN values with 0\n\n# Use one-hot encoding for categorical variables if needed:\n# train_data = pd.get_dummies(train_data, columns=['categorical_column'])\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": "\ntrain_labels = pd.read_csv('trainLabels.csv')\ntest_data = pd.read_csv('test.csv')\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 30
    },
    {
      "cell_type": "code",
      "source": "# Choose a suitable algorithm\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(random_state=42)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Merge the features and labels based on the 'id' column\nmerged_data = pd.merge(train_data, train_labels, on='id')\n\n# Split the merged data into features and labels\nX = merged_data.drop(columns=['id', 'y1', 'y2', 'y3', 'y4'])\ny = merged_data[['y1', 'y2', 'y3', 'y4']]\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": "print(\"Number of samples in train_data:\", train_data.shape[0])\nprint(\"Number of samples in train_labels:\", train_labels.shape[0])\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Number of samples in train_data: 7402\nNumber of samples in train_labels: 49999\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": "# Check for duplicates in train_data\nduplicate_rows = train_data[train_data.duplicated()]\nprint(\"Duplicate rows in train_data:\", duplicate_rows)\n\n# Check for duplicates in train_labels\nduplicate_labels = train_labels[train_labels.duplicated()]\nprint(\"Duplicate labels in train_labels:\", duplicate_labels)\n\n# Check for missing labels in train_labels\nmissing_labels = train_labels[train_labels.isnull().any(axis=1)]\nprint(\"Rows with missing labels in train_labels:\", missing_labels)\n\nmissing_ids = train_data[~train_data['id'].isin(train_labels['id'])]\nprint(\"Rows with missing IDs in train_data:\", missing_ids)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Duplicate rows in train_data: Empty DataFrame\nColumns: [id, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28, x29, x30, x31, x32, x33, x34, x35, x36, x37, x38, x39, x40, x41, x42, x43, x44, x45, x46, x47, x48, x49, x50, x51, x52, x53, x54, x55, x56, x57, x58, x59, x60, x61, x62, x63, x64, x65, x66, x67, x68, x69, x70, x71, x72, x73, x74, x75, x76, x77, x78, x79, x80, x81, x82, x83, x84, x85, x86, x87, x88, x89, x90, x91, x92, x93, x94, x95, x96, x97, x98, x99, ...]\nIndex: []\n\n[0 rows x 146 columns]\nDuplicate labels in train_labels: Empty DataFrame\nColumns: [id, y1, y2, y3, y4, y5, y6, y7, y8, y9, y10, y11, y12, y13, y14, y15, y16, y17, y18, y19, y20, y21, y22, y23, y24, y25, y26, y27, y28, y29, y30, y31, y32, y33]\nIndex: []\n\n[0 rows x 34 columns]\nRows with missing labels in train_labels: Empty DataFrame\nColumns: [id, y1, y2, y3, y4, y5, y6, y7, y8, y9, y10, y11, y12, y13, y14, y15, y16, y17, y18, y19, y20, y21, y22, y23, y24, y25, y26, y27, y28, y29, y30, y31, y32, y33]\nIndex: []\n\n[0 rows x 34 columns]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 31
    },
    {
      "cell_type": "code",
      "source": "unique_ids_data = train_data['id'].nunique()\nunique_ids_labels = train_labels['id'].nunique()\nprint(\"Unique IDs in train_data:\", unique_ids_data)\nprint(\"Unique IDs in train_labels:\", unique_ids_labels)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Unique IDs in train_data: 7402\nUnique IDs in train_labels: 49999\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": "merged_data = pd.merge(train_data, train_labels, on='id', how='inner')\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": "missing_ids = train_labels[~train_labels['id'].isin(train_data['id'])]['id']\nprint(\"IDs in train_labels but not in train_data:\", missing_ids)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "IDs in train_labels but not in train_data: 1            2\n8            9\n12          13\n25          26\n26          27\n         ...  \n49994    49995\n49995    49996\n49996    49997\n49997    49998\n49998    49999\nName: id, Length: 42597, dtype: int64\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": "# Merge train_data and train_labels on the 'id' column\nmerged_data = pd.merge(train_data, train_labels, on='id')\n\n# Display the first few rows of the merged data\nprint(merged_data.head())\n\n# Check the shape of the merged data\nprint(\"Shape of merged_data:\", merged_data.shape)\n\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "   id   x1  x2                                            x3  \\\n0   1   NO  NO  dqOiM6yBYgnVSezBRiQXs9bvOFnRqrtIoXRIElxD7g8=   \n1   3   NO  NO  ib4VpsEsqJHzDiyL0dZLQ+xQzDPrkxE+9T3mx5fv2wI=   \n2   4  YES  NO  BfrqME7vdLw3suQp6YAT16W2piNUmpKhMzuDrVrFQ4w=   \n3   5   NO  NO  RTjsrrR8DTlJyaIP9Q3Z8s0zseqlVQTrlSe97GCWfbk=   \n4   6   NO  NO  cz26ErvsEb3tZqeaIlmLIwjn8D7YVXYynSLV1WltcUk=   \n\n                                             x4        x5        x6        x7  \\\n0  GNjrXXA3SxbgD0dTRblAPO9jFJ7AIaZnu/f48g5XSUk=  0.576561  0.073139  0.481394   \n1  X6dDAI/DZOWvu0Dg6gCgRoNr2vTUz/mc4SdHTNUPS38=  1.341803  0.051422  0.935572   \n2  YGCdISifn4fLao/ASKdZFhGIq23oqzfSbUVb6px1pig=  0.653912  0.041471  0.940787   \n3  3yK2OPj1uYDsoMgsxsjY1FxXkOllD8Xfh20VYGqT+nU=  1.415919  0.000000  1.000000   \n4  olN1LoaeSyI8h+udI/jquozrw4R8YQ+cVwHq1dOUO5s=  1.413677  0.000000  1.000000   \n\n         x8        x9  ... y24 y25 y26 y27 y28  y29  y30  y31  y32  y33  \n0  0.115697  0.472474  ...   0   0   0   0   0    0    0    0    0    1  \n1  0.041440  0.501710  ...   0   0   0   0   0    0    0    0    0    0  \n2  0.090851  0.556564  ...   0   0   0   0   0    0    0    0    0    1  \n3  0.000000  0.375297  ...   0   0   0   0   0    0    0    0    0    1  \n4  0.000000  0.667724  ...   0   0   0   0   0    0    0    0    0    1  \n\n[5 rows x 179 columns]\nShape of merged_data: (7402, 179)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 32
    },
    {
      "cell_type": "code",
      "source": "X = merged_data.drop(['id', 'y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7', 'y8', 'y9', 'y10', 'y11', 'y12', 'y13', 'y14', 'y15', 'y16', 'y17', 'y18', 'y19', 'y20', 'y21', 'y22', 'y23', 'y24', 'y25', 'y26', 'y27', 'y28', 'y29', 'y30', 'y31', 'y32', 'y33'], axis=1)\ny = merged_data[['y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7', 'y8', 'y9', 'y10', 'y11', 'y12', 'y13', 'y14', 'y15', 'y16', 'y17', 'y18', 'y19', 'y20', 'y21', 'y22', 'y23', 'y24', 'y25', 'y26', 'y27', 'y28', 'y29', 'y30', 'y31', 'y32', 'y33']]\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": "from sklearn.preprocessing import LabelEncoder\n\n# Apply Label Encoding to categorical columns\nlabel_encoder = LabelEncoder()\nX_encoded = X.apply(label_encoder.fit_transform)\n\n# Continue with the train-test split and model training\nX_train_encoded, X_val_encoded, y_train, y_val = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n\n# Train the model\nmodel.fit(X_train_encoded, y_train)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "execution_count": 29,
          "output_type": "execute_result",
          "data": {
            "text/plain": "RandomForestClassifier(random_state=42)",
            "text/html": "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 29
    },
    {
      "cell_type": "code",
      "source": "# Make predictions on the validation set\n\nfrom sklearn.metrics import f1_score\n\ny_pred = model.predict(X_val_encoded)\n\n# Evaluate the model\nf1 = f1_score(y_val, y_pred, average='micro')\nprint(f\"F1 Score: {f1}\")\n\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "F1 Score: 0.848464619492657\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 21
    },
    {
      "cell_type": "code",
      "source": "# Make predictions on the validation set\ny_pred = model.predict(X_val_encoded)\n\n# Convert predictions to a DataFrame with dynamic column names\ncolumn_names = [f'y{i+1}' for i in range(y_pred.shape[1])]\npred_df = pd.DataFrame(data=y_pred, columns=column_names)\n\n# Add the 'id' column back to the predictions DataFrame\npred_df['id'] = X_val.index.values\n\n# Reorder columns to match the expected format\npred_df = pred_df[['id'] + column_names]\n\n# Save predictions to a CSV file\npred_df.to_csv('predictions.csv', index=False)\n\n# Evaluate the model\nf1 = f1_score(y_val, y_pred, average='micro')\nprint(f\"F1 Score: {f1}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "F1 Score: 0.848464619492657\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "code",
      "source": "# Assuming result_df has 'id_label' and 'pred' columns\nresult_df.to_csv('predictions_result.csv', index=False)\n\n# Load the sample submission file\nsample_submission = pd.read_csv('sampleSubmission_small.csv')\n\n# Merge the predictions with the sample submission to include all 'id_label' values\nfinal_result = pd.merge(sample_submission, result_df, on='id_label', how='left')\n\n# Fill any missing values in 'pred' column with 0\nfinal_result['pred'].fillna(0, inplace=True)\n\n# Save the final result to a CSV file\nfinal_result.to_csv('final_predictions.csv', index=False)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}